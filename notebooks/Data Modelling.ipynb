{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08ea094-0456-4bec-a32e-298ed7ae2cd0",
   "metadata": {},
   "source": [
    "# Baseline Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d95fec4-adda-4118-8e38-4bbe4aff0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b42de7b6-1618-43b7-bfe7-017e76400484",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet('../data/processed/processed_test.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9765d3cd-adb2-4524-a409-7fbf17d1076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "      <th>Premium_to_Age_Ratio</th>\n",
       "      <th>age_group_senior</th>\n",
       "      <th>age_group_young</th>\n",
       "      <th>Vehicle_Age_LT_1_Year</th>\n",
       "      <th>Vehicle_Age_GT_2_Years</th>\n",
       "      <th>Vehicle_Damage_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333777</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070366</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.748795</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.396751</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057496</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.342443</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.773246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.527181</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.521998</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.151783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.148985</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048348</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.581474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760095</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.633242</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046259</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-1.378580</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070132</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381104</th>\n",
       "      <td>1</td>\n",
       "      <td>2.267815</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.792954</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.829086</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381105</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.568774</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069551</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-0.279037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.711938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381106</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.148985</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.079509</td>\n",
       "      <td>0</td>\n",
       "      <td>1.275025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381107</th>\n",
       "      <td>0</td>\n",
       "      <td>1.881007</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078110</td>\n",
       "      <td>124.0</td>\n",
       "      <td>-0.960275</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.415730</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381108</th>\n",
       "      <td>1</td>\n",
       "      <td>0.462713</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072827</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.987826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381109 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Gender       Age Driving_License Region_Code Previously_Insured  \\\n",
       "0           1  0.333777               1        28.0                  0   \n",
       "1           1  2.396751               1         3.0                  0   \n",
       "2           1  0.527181               1        28.0                  0   \n",
       "3           1 -1.148985               1        11.0                  1   \n",
       "4           0 -0.633242               1        41.0                  1   \n",
       "...       ...       ...             ...         ...                ...   \n",
       "381104      1  2.267815               1        26.0                  1   \n",
       "381105      1 -0.568774               1        37.0                  1   \n",
       "381106      1 -1.148985               1        30.0                  1   \n",
       "381107      0  1.881007               1        14.0                  0   \n",
       "381108      1  0.462713               1        29.0                  0   \n",
       "\n",
       "        Annual_Premium Policy_Sales_Channel   Vintage  Response  \\\n",
       "0             0.070366                 26.0  0.748795         1   \n",
       "1             0.057496                 26.0  0.342443         0   \n",
       "2             0.066347                 26.0 -1.521998         1   \n",
       "3             0.048348                152.0  0.581474         0   \n",
       "4             0.046259                152.0 -1.378580         0   \n",
       "...                ...                  ...       ...       ...   \n",
       "381104        0.051234                 26.0 -0.792954         0   \n",
       "381105        0.069551                152.0 -0.279037         0   \n",
       "381106        0.060439                160.0  0.079509         0   \n",
       "381107        0.078110                124.0 -0.960275         0   \n",
       "381108        0.072827                 26.0  0.987826         0   \n",
       "\n",
       "        Premium_to_Age_Ratio age_group_senior age_group_young  \\\n",
       "0                   0.022331                0               0   \n",
       "1                  -0.773246                1               0   \n",
       "2                  -0.151783                0               0   \n",
       "3                   0.760095                0               1   \n",
       "4                   0.070132                0               1   \n",
       "...                      ...              ...             ...   \n",
       "381104             -0.829086                1               0   \n",
       "381105              0.711938                0               0   \n",
       "381106              1.275025                0               1   \n",
       "381107             -0.415730                1               0   \n",
       "381108              0.003673                0               0   \n",
       "\n",
       "       Vehicle_Age_LT_1_Year Vehicle_Age_GT_2_Years Vehicle_Damage_Yes  \n",
       "0                          0                      1                  1  \n",
       "1                          0                      0                  0  \n",
       "2                          0                      1                  1  \n",
       "3                          1                      0                  0  \n",
       "4                          1                      0                  0  \n",
       "...                      ...                    ...                ...  \n",
       "381104                     0                      0                  0  \n",
       "381105                     1                      0                  0  \n",
       "381106                     1                      0                  0  \n",
       "381107                     0                      1                  1  \n",
       "381108                     0                      0                  0  \n",
       "\n",
       "[381109 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('../data/processed/processed_train.parquet.gzip')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41f8f38-8bd7-4dea-b5a2-0147fe8b50b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 381109 entries, 0 to 381108\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   Gender                  381109 non-null  object \n",
      " 1   Age                     381109 non-null  float64\n",
      " 2   Driving_License         381109 non-null  object \n",
      " 3   Region_Code             381109 non-null  object \n",
      " 4   Previously_Insured      381109 non-null  object \n",
      " 5   Annual_Premium          381109 non-null  float64\n",
      " 6   Policy_Sales_Channel    381109 non-null  object \n",
      " 7   Vintage                 381109 non-null  float64\n",
      " 8   Response                381109 non-null  int64  \n",
      " 9   Premium_to_Age_Ratio    381109 non-null  float64\n",
      " 10  age_group_senior        381109 non-null  object \n",
      " 11  age_group_young         381109 non-null  object \n",
      " 12  Vehicle_Age_LT_1_Year   381109 non-null  object \n",
      " 13  Vehicle_Age_GT_2_Years  381109 non-null  object \n",
      " 14  Vehicle_Damage_Yes      381109 non-null  object \n",
      "dtypes: float64(4), int64(1), object(10)\n",
      "memory usage: 43.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadac01a-3539-42b8-bbf8-cad098dbe6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Response', axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e0ab27-6c6f-47fd-b787-d7ce188ed479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (304887, 14)\n",
      "Validation set shape: (76222, 14)\n",
      "Class distribution in training set:\n",
      "Response\n",
      "0    0.877437\n",
      "1    0.122563\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184493de-060d-4d67-af40-0d9a6f7d051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    # Predict both class labels and probabilities\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate various evaluation metrics\n",
    "    # ROC AUC: Good for imbalanced datasets, measures ability to distinguish between classes\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    # PR AUC: Also good for imbalanced datasets, focuses on positive class performance\n",
    "    pr_auc = average_precision_score(y, y_pred_proba)\n",
    "    # F1 Score: Harmonic mean of precision and recall\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    # Confusion Matrix: Gives a breakdown of correct and incorrect classifications\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Print results for easy comparison between models\n",
    "    print(f\"{model_name} Results:\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89296aee-b5dd-44a3-8633-ea5b3ba117da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance using SMOTE\n",
    "# SMOTE creates synthetic examples of the minority class to balance the dataset\n",
    "smote = SMOTE(random_state=13)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5604548d-611d-4208-be27-e1f6e1e82af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (535038, 14)\n",
      "Validation set shape: (76222, 14)\n",
      "Class distribution in training set:\n",
      "Response\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", X_train_resampled.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train_resampled.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8be524-6183-4afc-87f6-3daf1e420521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "ROC AUC: 0.8403\n",
      "PR AUC: 0.3251\n",
      "F1 Score: 0.4019\n",
      "Confusion Matrix:\n",
      "[[40734 26146]\n",
      " [  417  8925]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - Our baseline model\n",
    "# Simple, interpretable, and often performs well on linearly separable data\n",
    "lr_model = LogisticRegression(class_weight='balanced', random_state=13, max_iter=1000)\n",
    "# class_weight='balanced' adjusts weights inversely proportional to class frequencies\n",
    "lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "evaluate_model(lr_model, X_val, y_val, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae2c705-0d33-431a-b510-1c355ebc9cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "ROC AUC: 0.8327\n",
      "PR AUC: 0.3154\n",
      "F1 Score: 0.3782\n",
      "Confusion Matrix:\n",
      "[[58136  8744]\n",
      " [ 5125  4217]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=13)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "evaluate_model(rf_model, X_val, y_val, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98730a28-fba2-4198-8788-236dd0fe49d2",
   "metadata": {},
   "source": [
    "### Logistic Regression vs Random Forest\n",
    "\n",
    "The comparison between the Logistic Regression and **Random Forest models** reveals interesting insights into our insurance cross-sell prediction task. While both models show moderate predictive power, the Random Forest **slightly outperforms** the Logistic Regression in terms of ROC AUC (0.8327 vs 0.8403), indicating a better ability to distinguish between classes. \n",
    "\n",
    "However, the **Logistic Regression model** demonstrates a **higher precision-recall balance** with a superior F1 Score (0.4019 vs 0.3782). This suggests that for this particular dataset, the simpler linear model might be capturing some important linear relationships that the more complex Random Forest is potentially overfitting. \n",
    "\n",
    "The confusion matrices indicate that **both models struggle with false positives**, but the **Random Forest appears to have a lower false negative** rate, which could be valuable if **minimizing missed opportunities** is a **priority for the insurance company's marketing strategy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523193b-4857-42da-b30e-4be657b0abe7",
   "metadata": {},
   "source": [
    "### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2759eb63-d4d1-49a5-aa9a-1109227d474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Important Features:\n",
      "                  feature  importance\n",
      "13     Vehicle_Damage_Yes    0.183252\n",
      "1                     Age    0.161707\n",
      "4      Previously_Insured    0.159697\n",
      "7                 Vintage    0.117761\n",
      "8    Premium_to_Age_Ratio    0.106742\n",
      "5          Annual_Premium    0.082149\n",
      "3             Region_Code    0.059031\n",
      "6    Policy_Sales_Channel    0.049347\n",
      "11  Vehicle_Age_LT_1_Year    0.030532\n",
      "10        age_group_young    0.019255\n"
     ]
    }
   ],
   "source": [
    "# Feature importance for Random Forest\n",
    "# This helps us understand which features are most influential in making predictions\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False).head(10)\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d3a74-684a-4198-b07e-bccd28786b1b",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8a425b0-32a5-4848-979f-15c696ec2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = X_train_resampled.astype(float)\n",
    "X_val = X_val.astype(float)\n",
    "\n",
    "# Verify the conversion\n",
    "# print(X_train_resampled.dtypes)\n",
    "# print(X_val_scaled.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a8661e9-1b4a-4122-98fd-781b3620509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_resampled.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train_resampled.values)\n",
    "X_val_tensor = torch.FloatTensor(X_val.values)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9b50aac-74dd-44a2-ad70-dc8096ff95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e6db2d3-1485-4166-b0ac-1f4c28d9638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First hidden layer\n",
    "        self.bn1 = nn.BatchNorm1d(64)         # Batch Normalization\n",
    "        self.fc2 = nn.Linear(64, 32)          # Second hidden layer\n",
    "        self.bn2 = nn.BatchNorm1d(32)         # Batch Normalization\n",
    "        self.fc3 = nn.Linear(32, 1)           # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation for non-linearity\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd6d985b-0a17-4df4-8425-65361a2f45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Net(X_train_resampled.shape[1])\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Binary Cross Entropy loss is suitable for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "# Adam optimizer is generally a good default choice\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55992f51-196f-4bad-a895-621b7ddb471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4602\n",
      "Epoch [20/100], Loss: 0.4463\n",
      "Epoch [30/100], Loss: 0.4444\n",
      "Epoch [40/100], Loss: 0.4865\n",
      "Epoch [50/100], Loss: 0.4520\n",
      "Epoch [60/100], Loss: 0.4487\n",
      "Early stopping at epoch 59\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 30\n",
    "no_improve = 0\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    # Evaluate on validation set every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        \n",
    "    if no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632282d-1047-4f59-bdc2-80f27f15065e",
   "metadata": {},
   "source": [
    "### Training Loop Results\n",
    "\n",
    "The training loop results for the neural network model provide some interesting insights into its learning process. Over the course of 50 epochs, we can observe the validation loss fluctuating, but generally decreasing from an initial value of 0.4641 to 0.4255 by epoch 30. This indicates that the model is learning and improving its predictions on the validation set.\n",
    "\n",
    "However, there's a notable increase in loss towards the end, rising to 0.4982 by epoch 50. This uptick suggests that the model might be starting to overfit the training data, as its performance on the validation set begins to degrade. It's possible that the optimal number of training epochs for this model is around 30-40, where the validation loss was at its lowest. \n",
    "\n",
    "This observation could guide us in implementing early stopping or adjusting the model's complexity to prevent overfitting and achieve better generalization on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54702589-5c00-4d3d-8f5c-46dc7329977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchModelWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper class to make PyTorch models compatible with scikit-learn style evaluation.\n",
    "    This wrapper bridges the gap between PyTorch's functional API and scikit-learn's object-oriented API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, X_val):\n",
    "        \"\"\"\n",
    "        Initialize the wrapper with a PyTorch model and validation data.\n",
    "        \n",
    "        :param model: A trained PyTorch model\n",
    "        :param X_val: Validation data (can be DataFrame or numpy array)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        # Convert DataFrame to numpy array if necessary\n",
    "        # This ensures compatibility regardless of input type\n",
    "        self.X_val = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "        self.val_outputs = None\n",
    "        self._compute_outputs()\n",
    "\n",
    "    def _compute_outputs(self):\n",
    "        \"\"\"\n",
    "        Precompute the model outputs for the validation set.\n",
    "        This is done once to avoid redundant computations in predict and predict_proba.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            X_val_tensor = torch.FloatTensor(self.X_val)  # Convert to PyTorch tensor\n",
    "            # Compute and store the raw model outputs\n",
    "            self.val_outputs = self.model(X_val_tensor).squeeze().numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return class predictions (0 or 1) based on a threshold of 0.5.\n",
    "        \n",
    "        :param X: Input data (ignored as we use precomputed outputs)\n",
    "        :return: Class predictions as a numpy array\n",
    "        \"\"\"\n",
    "        # Convert raw outputs to class predictions\n",
    "        return (self.val_outputs > 0.5).astype(float)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for both classes.\n",
    "        \n",
    "        :param X: Input data (ignored as we use precomputed outputs)\n",
    "        :return: Probability estimates as a 2D numpy array\n",
    "        \"\"\"\n",
    "        # Create a 2D array with probabilities for both classes\n",
    "        return np.column_stack((1 - self.val_outputs, self.val_outputs))\n",
    "\n",
    "# Wrap our PyTorch model\n",
    "wrapped_model = PyTorchModelWrapper(model, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3e23b87-a513-486d-9dac-a675700d7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7239\n",
      "Neural Network Results:\n",
      "ROC AUC: 0.8465\n",
      "PR AUC: 0.3455\n",
      "F1 Score: 0.4350\n",
      "Confusion Matrix:\n",
      "[[47073 19807]\n",
      " [ 1239  8103]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor).squeeze()\n",
    "    val_predictions = (val_outputs > 0.5).float()\n",
    "    accuracy = (val_predictions == y_val_tensor).float().mean()\n",
    "    print(f'Validation Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "# Additional evaluation using evaluate_model\n",
    "evaluate_model(wrapped_model, X_val.values, y_val.values, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0329f3-3227-4b3c-99bc-1deb620321c9",
   "metadata": {},
   "source": [
    "# Conclusion: Cross-Selling Vehicle Insurance to Health Insurance Customers\n",
    "\n",
    "### Model Performance Overview\n",
    "\n",
    "The analysis employs three distinct models to predict customer interest in vehicle insurance: Logistic Regression, Random Forest, and a Neural Network. Each model offers unique insights into customer behavior and preferences.\n",
    "\n",
    "### Neural Network: The Slight Edge\n",
    "The Neural Network emerges as the top performer with an ROC AUC of 0.8465 and an impressive accuracy of 72.39%. This suggests that complex, non-linear relationships within the data are captured effectively by this model. However, its slight edge over simpler models raises questions about the trade-off between performance and interpretability.\n",
    "\n",
    "### Logistic Regression: Simplicity Meets Effectiveness\n",
    "Close behind, the Logistic Regression model achieves an ROC AUC of 0.8403. Its standout feature is the highest F1 Score of 0.4019, indicating a well-balanced precision and recall. This balance is crucial for optimizing marketing efforts, ensuring we capture a good proportion of interested customers without excessive false positives.\n",
    "\n",
    "### Random Forest: Robust but Not Superior\n",
    "The Random Forest model, while robust, doesn't outperform its counterparts with an ROC AUC of 0.8327. However, its ability to handle non-linear relationships and feature interactions shouldn't be overlooked in our overall strategy.\n",
    "\n",
    "### Business Insights and Strategic Implications\n",
    "The similar performance across models unveils several key insights for our insurance cross-selling strategy:\n",
    "\n",
    "1. Targeted Marketing Potential: With prediction accuracies exceeding 70%, we have a solid foundation for more targeted marketing campaigns. This precision allows for significant improvements in marketing efficiency and potential cost savings.\n",
    "2. Conservative Prediction Approach: All models demonstrate a tendency towards false positives rather than false negatives. In practical terms, this means we're more likely to market to uninterested customers than to miss potentially interested ones. While this ensures comprehensive coverage, it also suggests room for fine-tuning to reduce unnecessary marketing spend.\n",
    "3. Customer Segmentation Opportunities: The models' predictive capabilities enable more sophisticated customer segmentation. We can now tailor our communication strategies and product offerings based on the likelihood of interest in vehicle insurance, potentially leading to higher conversion rates and customer satisfaction.\n",
    "4. Balancing Complexity and Interpretability: While the Neural Network shows slightly superior performance, the Logistic Regression model offers a compelling balance between predictive power and interpretability. This balance is crucial for stakeholder buy-in and for deriving actionable insights from the model's predictions.\n",
    "\n",
    "## Recommended Action Plan\n",
    "Given these insights, we propose the following action plan:\n",
    "\n",
    "1. **Implement Logistic Regression Model**: Deploy the Logistic Regression model as our primary predictive tool. Its balance of performance and interpretability makes it ideal for initial implementation and stakeholder communication.\n",
    "2. **Develop Tiered Marketing Approach**: Create a tiered marketing strategy based on predicted interest levels. High-probability customers receive more personalized, high-touch approaches, while lower-probability customers receive more general, cost-effective communications.\n",
    "3. **Continuous Model Refinement**: Regularly update and refine the model with new data. Consider A/B testing different models in real-world scenarios to validate their performance beyond mere statistical metrics.\n",
    "4. **Investigate Feature Importance**: Conduct a deep dive into the features driving predictions in the Logistic Regression model. Use these insights to inform product development and marketing messaging.\n",
    "5. **Cross-Functional Collaboration**: Foster collaboration between the data science team and marketing department to ensure model insights are effectively translated into actionable marketing strategies.\n",
    "\n",
    "By implementing this data-driven approach, we position ourselves to significantly enhance our cross-selling effectiveness, optimize our marketing expenditure, and ultimately drive growth in our vehicle insurance portfolio among our existing health insurance customer base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
